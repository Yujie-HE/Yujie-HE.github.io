<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on YUJIE HE</title>
    <link>https://yujie-he.github.io/project/</link>
    <description>Recent content in Projects on YUJIE HE</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Last updated on Oct., 2021 · Yujie HE &amp;copy; 2019 - 2021</copyright>
    <lastBuildDate>Thu, 17 Jun 2021 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://yujie-he.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Development of vision based algorithms to a window/balcony drone delivery</title>
      <link>https://yujie-he.github.io/project/2021-lis-drone-delivery/</link>
      <pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2021-lis-drone-delivery/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Semester Research Student&lt;/strong&gt; at &lt;a href=&#34;https://lis.epfl.ch/&#34; target=&#34;_blank&#34;&gt;Laboratory of Intelligent Systems (LIS)&lt;/a&gt;, EPFL since &lt;em&gt;Feb. 2021&lt;/em&gt;
Supervisor: &lt;a href=&#34;https://people.epfl.ch/valentin.wueest/?lang=en&#34; target=&#34;_blank&#34;&gt;Valentin Wüest&lt;/a&gt; (PhD student), &lt;a href=&#34;https://people.epfl.ch/przemyslaw.kornatowski/?lang=en&#34; target=&#34;_blank&#34;&gt;Dr. Przemyslaw Mariusz Kornatowski&lt;/a&gt;, and &lt;a href=&#34;https://people.epfl.ch/dario.floreano&#34; target=&#34;_blank&#34;&gt;Prof. Dario Floreano&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;At the Laboratory of Intelligent Systems (LIS), passionate researchers are developing a human-friendly drone delivery system for last-cm delivery - &lt;a href=&#34;http://dronistics.epfl.ch&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Dronistics&lt;/strong&gt;&lt;/a&gt;. The system is composed of a safe drone called &lt;strong&gt;PackDrone&lt;/strong&gt; and software to control and monitor drones in real-time.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
  &lt;!-- &lt;img src=&#34;gearquad.jpg&#34; alt=&#34;gearquad_parcel&#34; style=&#34;zoom:12%;&#34; /&gt; --&gt;
  &lt;img src=&#34;https://dronistics.epfl.ch/img/PackDrone_deployed.jpg&#34; alt=&#34;PackDrone_deployed&#34;  width=&#34;200&#34;  /&gt;
  &lt;small&gt;
  &lt;b&gt;
    &lt;!-- Parcel placed above the cage allows the drone to transport parcels of various sizes without negative impact on lift --&gt;
     PackDrone can eliminate the damage from propellers or rotor blades with a foldable protective cage
  &lt;/b&gt;
  [Source: &lt;a href=&#34;http://dronistics.epfl.ch&#34; target=&#34;_blank&#34;&gt;Dronistics&lt;/a&gt;]
  &lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is the goal of this semester project?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our goal is to &lt;strong&gt;deliver to a balcony/window&lt;/strong&gt; which is &lt;strong&gt;tagged with a special symbol/pattern&lt;/strong&gt;. Moreover, the drone should be equipped with a system of &lt;strong&gt;collision avoidance&lt;/strong&gt; to prevent hitting a building.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Why motivates us to work on this project?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One vivid example is that in this special period of Covid-19, people are required to keep social distance while delivery work keeps operating. In contrast to large aircraft, window/balcony delivery with lightweight drone is a reasonable and effective solution to send valuable parcels such as medical supplies rapidly and safely.&lt;/p&gt;

&lt;h2 id=&#34;system-architecture&#34;&gt;System architecture&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Illustration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;./featured.jpg&#34; alt=&#34;experimental_drone&#34; width=&#34;600&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;./system_arch.jpg&#34; alt=&#34;system_arch&#34; width=&#34;600&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;
  &lt;small&gt;
  &lt;b&gt;System architecture of the proposed drone delivery system&lt;/b&gt;
  &lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Visual fiducial marker evaluation
&lt;img src=&#34;./tag_evaluation.jpg&#34; alt=&#34;tag_evaluation&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Onboard test&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;./tag_drone_real.png&#34; alt=&#34;tag_drone_real&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;final-presentation&#34;&gt;Final presentation&lt;/h2&gt;

&lt;!-- &lt;iframe src=&#34;https://drive.google.com/file/d/1LCtTQ2NFRRjhwrPHcfao5ApY6hocKZaQ/preview&#34; width=&#34;700&#34; height=&#34;400&#34;&gt;&lt;/iframe&gt; --&gt;

&lt;iframe src=&#34;https://drive.google.com/file/d/1VmY0fp5KuiljASgDYkci4Nhcj0Mt3HlK/preview&#34; width=&#34;700&#34; height=&#34;400&#34;&gt;&lt;/iframe&gt;

&lt;h2 id=&#34;code-videos&#34;&gt;Code &amp;amp; videos&lt;/h2&gt;

&lt;p&gt;🚧 To be updated!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Autonomous Navigation and Landing for Crazyflie</title>
      <link>https://yujie-he.github.io/project/2021-crazyflie-auto-nav/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2021-crazyflie-auto-nav/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Final project&lt;/strong&gt; in &lt;a href=&#34;https://edu.epfl.ch/coursebook/fr/aerial-robotics-MICRO-502&#34; target=&#34;_blank&#34;&gt;MICRO-502 Aerial robotics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Members: Yujie He, &lt;a href=&#34;https://github.com/Jianhao-zheng/&#34; target=&#34;_blank&#34;&gt;Jianhao Zheng&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kevinxqiu&#34; target=&#34;_blank&#34;&gt;Longlai Qiu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lecturer:  &lt;a href=&#34;https://people.epfl.ch/dario.floreano/?lang=en&#34; target=&#34;_blank&#34;&gt;Prof. Dario Floreano&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;goal-autonomous-navigation-and-landing-for-crazyflie&#34;&gt;Goal: Autonomous Navigation and Landing for Crazyflie&lt;/h2&gt;

&lt;p&gt;In this practical, we programed based on &lt;a href=&#34;https://www.bitcraze.io/products/crazyflie-2-1/&#34; target=&#34;_blank&#34;&gt;Crazyflie 2.1&lt;/a&gt; to find and precisely land on a platform with height of 10 cm by utilizing z reading from &lt;a href=&#34;https://www.bitcraze.io/products/flow-deck-v2/&#34; target=&#34;_blank&#34;&gt;flow deck&lt;/a&gt;. Additionally, We also utilized sensor readings from &lt;a href=&#34;https://www.bitcraze.io/products/multi-ranger-deck/&#34; target=&#34;_blank&#34;&gt;multi-ranger deck&lt;/a&gt; to avoid the obstacles presented in the environment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/blob/master/pics/cover.jpg&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/raw/master/pics/cover.jpg&#34; alt=&#34;cover&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Autonomous navigation &amp;amp; landing&lt;/th&gt;
&lt;th&gt;Workflow&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;✓ Local obstacle avoidance &lt;br&gt;✓ Grid-based coverage path planning &lt;br&gt;✓ Waypoint following &lt;br&gt;✓ A* search-based re-planning&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/master/pics/pipeline_final.png&#34; alt=&#34;pipeline_final&#34;  width=&#34;500&#34;/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modular library for different tasks&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;├── cf_load_params.py  # parameter setting
├── cf_search.py       # searching functions such as, coverage planning, box edge detection, A* search
├── cf_state_class.py  # state estimation class for the proposed task
└── cf_utilis.py       # utility functions, such as live plotting
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Utilized &lt;code&gt;argparse&lt;/code&gt; for quick parameter adjustment and tuning&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Utilized &lt;code&gt;matplotlib&lt;/code&gt; for real-time visualization&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;structure&#34;&gt;Structure&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Code folder: &lt;a href=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/tree/master/code/crazyflie-lib-python/group_7&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;./code/crazyflie-lib-python/group_7/&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;.
├── cf_load_params.py
├── cf_search.py
├── cf_state_class.py
├── cf_utilis.py
├── overall.py
├── draw_traj_demo.py
├── logs
│   ├── overall-20210530_1930_x.csv
│   ├── overall-20210530_1930_x_half.csv
│   ├── overall-20210530_1930_y.csv
│   └── overall-20210530_1930_y_half.cs
└── readme.md
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;demo&#34;&gt;Demo&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;overall.py&lt;/code&gt;: overall pipeline from taking off to landing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -x (float) for setting initial x position
# -y (float) for setting initial y position
# -v (bool) for enabling visualization
python overall.py -x 0.6 -y 0.6 -v
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/blob/master/pics/cf_land.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/raw/master/pics/cf_land.gif&#34; alt=&#34;cf_land&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;draw_traj.py&lt;/code&gt;: x-y trajectory visualization with region annotation&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# --log_folder (str) for assigning input log folder
# --logname (str) for loding log file
# --img_folder (str) for assigning output image folder
# -n/--name (str) for assigning output image name
# --zone_anno (bool) for enabling region annotation
python draw_traj_demo.py --logname overall-20210530_1930 -n cf_demo --zone_anno
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/blob/master/pics/cf_demo.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/raw/master/pics/cf_demo.png&#34; alt=&#34;cf_demo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The estimated values drift considerably after long flights. Moreover, the predicted starting position is significantly different from the starting point after the drone re-takes off.&lt;/p&gt;

&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Features&lt;/th&gt;
&lt;th&gt;Figures&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;✓ Size: 480 cm (W) × 120 cm (H) &lt;/br&gt;✓ Starting &amp;amp; Landing pad&lt;/br&gt; - starting (x, y) = (60 cm, 60 cm)&lt;/br&gt; - landing pad randomly placed &lt;/br&gt;✓ Circular and rectangular obstacles&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/master/pics/experimental_setup.png&#34; alt=&#34;experimental_setup&#34;  width=&#34;500&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;p&gt;&lt;center&gt;
&lt;iframe width=&#34;560&#34; height=&#34;320&#34; src=&#34;https://www.youtube.com/embed/RP4-SlhOIUk&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;slide&#34;&gt;Slide&lt;/h2&gt;

&lt;p&gt;&lt;center&gt;
&lt;iframe src=&#34;https://drive.google.com/file/d/1vY_UMflVXOcUSOASHkGHsSTXCBmwrVhK/preview&#34; width=&#34;560&#34; height=&#34;320&#34;&gt;&lt;/iframe&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sim2Real Development for Thymio with ROS</title>
      <link>https://yujie-he.github.io/project/2021-ros-basics/</link>
      <pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2021-ros-basics/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://edu.epfl.ch/coursebook/fr/robotics-practicals-MICRO-453&#34; target=&#34;_blank&#34;&gt;MICRO-453 Robotics practicals&lt;/a&gt;, EPFL, 2021 Spring&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Students: &lt;a href=&#34;https://github.com/Chuanfang-Neptune&#34; target=&#34;_blank&#34;&gt;Chuanfang Ning&lt;/a&gt;, &lt;a href=&#34;https://github.com/Jianhao-zheng&#34; target=&#34;_blank&#34;&gt;Jianhao Zheng&lt;/a&gt;, and Yujie He&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;keywords&#34;&gt;🔑 Keywords&lt;/h2&gt;

&lt;p&gt;Thymio, PID, Way following, Obstacle avoidance, Pledge algorithm, ArUco marker, Sim2Real, Gazebo&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./sim2real.png&#34; alt=&#34;sim2real&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;small&gt;
&lt;strong&gt;The tested environment in Gazebo and real-world setup&lt;/strong&gt;
&lt;/small&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;example-videos&#34;&gt;📷 Example videos&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;For more examples, please refer to &lt;a href=&#34;https://go.epfl.ch/ros_basics_final_2021&#34; target=&#34;_blank&#34;&gt;https://go.epfl.ch/ros_basics_final_2021&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Simulation in Gazebo&lt;/th&gt;
&lt;th&gt;Real-world test on campus&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;iframe width=&#34;340&#34; height=&#34;200&#34; src=&#34;https://www.youtube.com/embed/_3xvN2QztKM&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/td&gt;
&lt;td&gt;&lt;iframe width=&#34;340&#34; height=&#34;200&#34; src=&#34;https://www.youtube.com/embed/Ydh_I8mSHz4&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;how-to-launch-the-example-code&#34;&gt;🔨 How to launch the example code?&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;For more details, please refer to &lt;a href=&#34;https://github.com/hibetterheyj/EPFL_ROS_Practicals_Project/&#34; target=&#34;_blank&#34;&gt;hibetterheyj/&lt;strong&gt;EPFL_ROS_Practicals_Project&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;simulate Thymio robot in Gazebo with an interactive window&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  roslaunch ros_basics_control simu_thymio.launch
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;adding waypoints and obstacle for the robot&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  roslaunch ros_basics_control simu_thymio.launch
  roslaunch ros_basics_exercise set_simu_waypoints_obstacle.launch
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;extract pose and sensor information from rosbag files&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  roslaunch ros_basics_exercise view_with_rosbag.launch
  # open a new terminal
  rosrun ros_basics_exercise topic_reader.py
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;plot trajectory comparison between real and simulation (using matlab)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  cd results_from_bag/
  # run `plot_traj_comp.m` in MATLAB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hibetterheyj/EPFL_ROS_Practicals_Project/master/results_from_bag/traj_thymio_simulation_navigation_with_obstacle_avoidance.png&#34; alt=&#34;traj_thymio_simulation_navigation_with_obstacle_avoidance&#34; style=&#34;zoom:30%;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;acknowledgement&#34;&gt;⭐️ Acknowledgement&lt;/h2&gt;

&lt;p&gt;Thanks to Vaios Papaspyros and Rafael Barmak from MOBOTS at EPFL for the amazing course tutorials !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online Visual Object Tracking for UAV in Dynamic Environments</title>
      <link>https://yujie-he.github.io/project/2020-tracking4uav/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2020-tracking4uav/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Undergraduate Research Assistant&lt;/strong&gt; since &lt;em&gt;Sep. 2018&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Investigated correlation filter (CF)-based &lt;strong&gt;visual object tracking&lt;/strong&gt; for unmanned aerial vehicles. By applying &lt;strong&gt;machine learning &amp;amp; deep learning&lt;/strong&gt; techniques, we have improved the existing trackers on overall tracking performance in challenging scenarios with real-time operational capability.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;papers-with-code&#34;&gt;Papers with code&lt;/h2&gt;

&lt;p&gt;Related work has been published in journals and conferences as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Proposed a lightweight and generalizable &lt;strong&gt;triple attention strategy&lt;/strong&gt; on CF-based framework by exploiting mutual independence of the appearance model and feature responses to implement real-time tracking for UAV.&lt;/p&gt;

&lt;p&gt;🚩 &lt;a href=&#34;../../publication/2020_tacf_iros/&#34;&gt;&lt;em&gt;Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters&lt;/em&gt;&lt;/a&gt; in &lt;strong&gt;IROS 2020&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Employed the adaptive &lt;strong&gt;GMSD-based context analysis&lt;/strong&gt; and &lt;strong&gt;dynamic weighted filters&lt;/strong&gt; for utilizing both contextual and historical information, and leveraged &lt;strong&gt;lightweight convolution features&lt;/strong&gt; to efficiently raise the tracking robustness.&lt;/p&gt;

&lt;p&gt;🚩 &lt;a href=&#34;../../publication/2020_mkct_ncaa/&#34;&gt;&lt;em&gt;Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters&lt;/em&gt;&lt;/a&gt; in &lt;strong&gt;Neural Computing and Applications&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Exploited the inter-frame information between prediction and backtracking phases for further incorporating the &lt;strong&gt;bidirectional incongruity error&lt;/strong&gt; into the CF learning.&lt;/p&gt;

&lt;p&gt;🚩 &lt;a href=&#34;../../publication/2020_bicf_icra/&#34;&gt;&lt;em&gt;BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking&lt;/em&gt;&lt;/a&gt; in &lt;strong&gt;ICRA 2020&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For more info, please refer to my &lt;a href=&#34;https://www.youtube.com/channel/UCGpK01NL0j3RkXpsODXm-Dg&#34; target=&#34;_blank&#34;&gt;YouTube channel&lt;/a&gt; and &lt;a href=&#34;https://github.com/hibetterheyj&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>3D Zonal Segmentation of the Prostate MRI (Deep Learning Final Project)</title>
      <link>https://yujie-he.github.io/project/2019-dl-mip/</link>
      <pubDate>Tue, 07 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2019-dl-mip/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Final project for TJ-100685 深度学习 | Deep learning&lt;/strong&gt;, &lt;em&gt;Sep. 2019 - Jan. 2020&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Utilized the latest &lt;strong&gt;Weight Standardization&lt;/strong&gt;}** (WS) as well as &lt;strong&gt;GroupNorm&lt;/strong&gt; to accelerate neural networks training from scratch for 3D Zonal Segmentation of the &lt;strong&gt;Prostate MRI images&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Conducted extensive evaluation between the proposed UWG-Net with the baseline with &lt;strong&gt;small batch sizes&lt;/strong&gt;, achieving 2-3\% increase in &lt;strong&gt;multi-class segmentation accuracy&lt;/strong&gt; for medical imaging application.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more info, you can refer to this &lt;a href=&#34;https://yujie-he.github.io/study/2019-deep-learning/final_project/&#34; target=&#34;_blank&#34;&gt;page&lt;/a&gt; !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching Assistant in Open Source Hardware and Programming</title>
      <link>https://yujie-he.github.io/project/2019-tongji-ta/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2019-tongji-ta/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Teaching Assistant&lt;/strong&gt;, &lt;em&gt;Sep. 2018 - Jan. 2019&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Assisted first-year students major in Industrial Design to get started Arduino Hardware and Programming&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Designed a series of the electromechanical modules for Industrial Design students&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gave lectures on basic mechanical principle with Arduino hardware and programming and advanced RGBD sensors for the semester project&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following video is &lt;strong&gt;Mechatronics Module Experiments&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;iframe width=&#34;645&#34; height=&#34;360&#34; src=&#34;https://www.youtube.com/embed/gTo2n-7T-Ao&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DIAN Racing Formula Student Electric Team</title>
      <link>https://yujie-he.github.io/project/2018-dian-racing/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2018-dian-racing/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Powertrain Group Leader&lt;/strong&gt;, &lt;em&gt;Sep. 2016 - Dec. 2018&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Designed and optimized the overall powertrain system to ensure China&amp;rsquo;s first leading four-wheel-drive Formula Student Racecar, achieving an 8\% efficiency and 10\% lightweight improvement&lt;/li&gt;
&lt;li&gt;Participated FSEC 2017 - 2018 and SFJ 2018 as Chief Powertrain Engineer, contributing to DIAN Racing‘s win in first place in Engineering Design and Efficiency Prize, and Best Powertrain Award from 2017 to 2018&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following video is &lt;strong&gt;virtual assembly of DRe18&lt;/strong&gt;，which was what I worked for as Powertrain Group Leader.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;iframe width=&#34;645&#34; height=&#34;360&#34; src=&#34;https://www.youtube.com/embed/bWmHDvBw1qw&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;./design-report-FSEC19.jpg&#34; alt=&#34;design-report-FSEC19&#34; /&gt;
&lt;small&gt; &lt;strong&gt;Design Report Final @ FSEC19&lt;/strong&gt;&lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SLAM and Autonomous Navigation for Skid Steer Wheel Robot</title>
      <link>https://yujie-he.github.io/project/2018-hesai-internship/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2018-hesai-internship/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Robotics Algorithm Development Intern&lt;/strong&gt;, &lt;em&gt;Jul. 2018 - Aug. 2018&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Implemented sensor fusion between a 40-channel LiDAR, i.e., &lt;a href=&#34;https://www.hesaitech.com/pandora.html/Pandar40&#34; target=&#34;_blank&#34;&gt;Pandar40&lt;/a&gt; and gyroscope and achieved a 5% accuracy improvements on top of state-of-the-art SLAM framework and drew a 3D point cloud map of Tongji University Jiading Campus below 10m&lt;/li&gt;
&lt;li&gt;Deployed control, decision, and communication algorithms for a self-developed skid steer wheel robot, realizing autonomous navigation and obstacle avoidance in a $ 300 m^2 $ workspace&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;demos&#34;&gt;Demos&lt;/h2&gt;

&lt;p&gt;Examples of final mapping results can be seen as follows:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./tongjiFront_optimize.gif&#34; alt=&#34;tongjiFront_optimize&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./KWG_optimize.gif&#34; alt=&#34;KWG_optimize&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;
  &lt;strong&gt;Pointcloud Demo of Tongji Jiading Campus&lt;/br&gt;
  👉 &lt;a href=&#34;https://goo.gl/maps/ygsUXZUryBs2RFw2A&#34; target=&#34;_blank&#34;&gt;Corresponding satellite map from Google map&lt;/a&gt;&lt;/strong&gt;&lt;/br&gt;
  Up: &lt;strong&gt;Main Gate&lt;/strong&gt;, Down: &lt;strong&gt;Kaiwu Building&lt;/strong&gt;
 &lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;misc&#34;&gt;Misc.&lt;/h2&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./indoor-slam.jpg&#34; alt=&#34;indoor-slam-result&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt; &lt;strong&gt;Indoor SLAM @ Hesai Tech&lt;/strong&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./SSWR.jpg&#34; alt=&#34;algorithm-debugging&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt; &lt;strong&gt;Skid Steer Wheel Robot equipped with Pandar40 LiDAR&lt;/strong&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Super Power Robot Team</title>
      <link>https://yujie-he.github.io/project/2018-super-power/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2018-super-power/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Project Manager &amp;amp; Mechanical Development Leader&lt;/strong&gt;, &lt;em&gt;Oct. 2016 - Jun. 2018&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Designed two main robots to participate in national mobile robot competition, RoboMaster, achieving lightweight and stability of the chassis and 3DOF pan-tilt mechanism and multi-robot interaction&lt;/li&gt;
&lt;li&gt;Optimized structural design to enhance operation stability and achieve lightweight, enable the robots flexible operation and combating under complicated circumstances&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
