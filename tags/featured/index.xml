<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>FeaturedðŸ’¡ on YUJIE HE</title>
    <link>https://yujie-he.github.io/tags/featured/</link>
    <description>Recent content in FeaturedðŸ’¡ on YUJIE HE</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>Last updated on Feb., 2022 Â· Yujie HE &amp;copy; 2019 - 2022</copyright>
    <lastBuildDate>Tue, 01 Feb 2022 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://yujie-he.github.io/tags/featured/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Multi-modal pedestrian behavior analysis for Qolo robot</title>
      <link>https://yujie-he.github.io/project/2021-qolo-pedestrian-analysis/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2021-qolo-pedestrian-analysis/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Research Assistant&lt;/strong&gt; at &lt;a href=&#34;https://lasa.epfl.ch/&#34; target=&#34;_blank&#34;&gt;Learning Algorithms and  Systems Laboratory (LASA)&lt;/a&gt;, EPFL since &lt;em&gt;Oct. 2021&lt;/em&gt;
Supervisor: &lt;a href=&#34;https://people.epfl.ch/diego.paez&#34; target=&#34;_blank&#34;&gt;Dr. Diego Felipe Paez Granados&lt;/a&gt;, and &lt;a href=&#34;https://people.epfl.ch/aude.billard?lang=en&#34; target=&#34;_blank&#34;&gt;Prof. Aude Billard&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;

&lt;p&gt;&lt;center&gt;
  &lt;img src=&#34;./qolo_tracking.gif&#34; alt=&#34;Qolo trajectory and tracked pedestrian in world frame&#34;  width=&#34;90%&#34;  /&gt;
  &lt;small&gt;
  &lt;b&gt;
     Qolo trajectory and tracked pedestrian in world frame
  &lt;/b&gt;
  &lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;In this work, we will create a dataset of mobile robot navigation around pedestrians from experimental data of a personal mobility device navigating autonomously around pedestrians in the streets of center Lausanne.&lt;/p&gt;

&lt;p&gt;The focus will be to assess people navigation behavior around the robot by extracting trajectories and motions. I aim to build a detecting, tracking, and motion profile extraction pipeline on lidar and camera data.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
  &lt;img src=&#34;./featured.png&#34; alt=&#34;dataset_qolo_overview&#34;  width=&#34;60%&#34;  /&gt;
  &lt;small&gt;
  &lt;b&gt;
    Overview of detected pedestrian from recorded rosbag and qolo robot
  &lt;/b&gt;
  &lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;single-sequence-evaluation&#34;&gt;Single sequence evaluation&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metrics&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Crowd characteristics&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;./crowd_density.png&#34; alt=&#34;dataset_qolo_overview&#34;  width=&#34;80%&#34;  /&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Path efficiency&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;./qolo_path.png&#34; alt=&#34;qolo_path&#34;  width=&#34;70%&#34;  /&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Shared control performance&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;./qolo_command.png&#34; alt=&#34;qolo_command&#34;  width=&#34;85%&#34;  /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;interclass-evaluation&#34;&gt;Interclass evaluation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;./comp_path.png&#34; alt=&#34;comp_path&#34;  width=&#34;85%&#34;  /&gt;&lt;/p&gt;

&lt;h2 id=&#34;dataset-and-toolkit-overview&#34;&gt;Dataset and toolkit overview&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Code available in &lt;a href=&#34;https://github.com/epfl-lasa/crowdbot-evaluation-tools&#34; target=&#34;_blank&#34;&gt;epfl-lasa/&lt;strong&gt;crowdbot-evaluation-tools&lt;/strong&gt;&lt;/a&gt;!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;./single_frame_aggregate.jpg&#34; alt=&#34;single_frame_aggregate&#34;  width=&#34;95%&#34;  /&gt;
&lt;center&gt;
&lt;center&gt;
Trajectory of qolo with detected pedestrians
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monocular Visual Odometry Pipeline</title>
      <link>https://yujie-he.github.io/project/2022-visual-odometry/</link>
      <pubDate>Sun, 09 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2022-visual-odometry/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Final project&lt;/strong&gt; in &lt;a href=&#34;https://rpg.ifi.uzh.ch/teaching.html&#34; target=&#34;_blank&#34;&gt;UZH-DINF2039 Vision Algorithms for Mobile Robotics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Members: Yujie He and &lt;a href=&#34;https://github.com/Jianhao-zheng/&#34; target=&#34;_blank&#34;&gt;Jianhao Zheng&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lecturer:  &lt;a href=&#34;https://rpg.ifi.uzh.ch/people_scaramuzza.html&#34; target=&#34;_blank&#34;&gt;Prof. Dr. Davide Scaramuzza&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Mini project for &lt;a href=&#34;http://rpg.ifi.uzh.ch/teaching.html&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Vision Algorithms for Mobile Robots&lt;/em&gt;&lt;/a&gt; given by &lt;a href=&#34;http://rpg.ifi.uzh.ch/people_scaramuzza.html&#34; target=&#34;_blank&#34;&gt;Prof. Davide Scaramuzza&lt;/a&gt;, 2021&lt;/p&gt;

&lt;p&gt;Implementation of a working, simple, monocular visual odometry (VO) pipeline with the following implementation details:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;KLT-based or descriptor matching for bootstrapping initialization&lt;/li&gt;
&lt;li&gt;KLT Tracking of feature points across frames following by RANSAC&lt;/li&gt;
&lt;li&gt;Triangulation of new landmarks&lt;/li&gt;
&lt;li&gt;Local pose refinement through optimization&lt;/li&gt;
&lt;li&gt;Bundle adjustment for better pose estimation&lt;/li&gt;
&lt;li&gt;Release two &lt;a href=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline#custom-datasets&#34; target=&#34;_blank&#34;&gt;custom sequences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;demo&#34;&gt;Demo&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://youtube.com/playlist?list=PLisWEer2ynw1Ws1_km6y-xXDAIyvJ9weM&#34; target=&#34;_blank&#34;&gt;YouTube playlist - VAMR 2021Fall Mini Project - VO Demo - J. Zheng &amp;amp; Y. He&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Test sequences&lt;/th&gt;
&lt;th&gt;Demo&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;KITTI seq05&lt;/strong&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=ByywzaIwTSM&amp;amp;list=PLisWEer2ynw1Ws1_km6y-xXDAIyvJ9weM&amp;amp;index=1&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;] [&lt;a href=&#34;http://rpg.ifi.uzh.ch/docs/teaching/2021/kitti05.zip&#34; target=&#34;_blank&#34;&gt;data (1.4GB)&lt;/a&gt;]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/blob/master/gifs/kitti.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/raw/master/gifs/kitti.gif&#34; alt=&#34;epfl_parking&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;malaga seq07&lt;/strong&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=l-Jklm77tNg&amp;amp;list=PLisWEer2ynw1Ws1_km6y-xXDAIyvJ9weM&amp;amp;index=2&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;] [&lt;a href=&#34;http://rpg.ifi.uzh.ch/docs/teaching/2021/malaga-urban-dataset-extract-07.zip&#34; target=&#34;_blank&#34;&gt;data (4.4GB)&lt;/a&gt;]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/blob/master/gifs/malaga.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/raw/master/gifs/malaga.gif&#34; alt=&#34;epfl_parking&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;parking&lt;/strong&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=Xut0iuFSy8o&amp;amp;list=PLisWEer2ynw1Ws1_km6y-xXDAIyvJ9weM&amp;amp;index=3&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;] [&lt;a href=&#34;http://rpg.ifi.uzh.ch/docs/teaching/2021/parking.zip&#34; target=&#34;_blank&#34;&gt;data (208 MB)&lt;/a&gt;]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/blob/master/gifs/parking.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/raw/master/gifs/parking.gif&#34; alt=&#34;epfl_parking&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;epfl_parking&lt;/strong&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=eWNpX07L4_A&amp;amp;list=PLisWEer2ynw1Ws1_km6y-xXDAIyvJ9weM&amp;amp;index=4&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;] [&lt;a href=&#34;https://drive.google.com/file/d/1dWRwkZaB_mY21HzAGXzcXKBwg_2cBn5c/view&#34; target=&#34;_blank&#34;&gt;data (675 MB)&lt;/a&gt;]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/blob/master/gifs/epfl_parking.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/raw/master/gifs/epfl_parking.gif&#34; alt=&#34;epfl_parking&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;lausanne_center_nav&lt;/strong&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=qSgeN7ElPik&amp;amp;list=PLisWEer2ynw1Ws1_km6y-xXDAIyvJ9weM&amp;amp;index=5&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;] [&lt;a href=&#34;https://drive.google.com/file/d/1e1VCh19S4wyayX6s4J-8J6ad5YZx-B9a/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;data (58.8 MB)&lt;/a&gt;]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/blob/master/gifs/lausanne_center_nav.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline/raw/master/gifs/lausanne_center_nav.gif&#34; alt=&#34;epfl_parking&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;codebase&#34;&gt;Codebase&lt;/h2&gt;

&lt;h3 id=&#34;machine-specifications&#34;&gt;Machine specifications&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;CPU: AMD Ryzen 7 5800H, 3.2 GHz, 16 logical processors&lt;/li&gt;
&lt;li&gt;RAM: 16GB&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dev-environment&#34;&gt;Dev Environment&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Test passed&lt;/strong&gt; : matlab-2021b/matlab-2020b&lt;/li&gt;
&lt;li&gt;Toolbox used

&lt;ul&gt;
&lt;li&gt;Computer Vision Toolbox&lt;/li&gt;
&lt;li&gt;Image Processing Toolbox&lt;/li&gt;
&lt;li&gt;Optimization Toolbox&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-to-run&#34;&gt;How to run&lt;/h3&gt;

&lt;p&gt;Download dataset and copy them to the right folder. For details on setting data, please refer to &lt;a href=&#34;https://github.com/hibetterheyj/Visual-Odometry-Pipeline#data&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/a&gt;. To test the VO pipeline without bundle adjustment, run &lt;code&gt;main_demo.m&lt;/code&gt;. Change variable &lt;code&gt;ds&lt;/code&gt; to switch the testing dataset.&lt;/p&gt;

&lt;p&gt;For VO with bundle adjustment, plese run &lt;code&gt;main_BA.m&lt;/code&gt; and make sure &lt;code&gt;hyper_paras.is_BA&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;. (For now, only tested in parking dataset, ds = 2)&lt;/p&gt;

&lt;h3 id=&#34;folder-structure&#34;&gt;Folder Structure&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;Visual-Odometry-Pipeline/
â”œâ”€â”€ Continuous_operation # (matlab) implemented algorithms about continuous operation
â”œâ”€â”€ Initialization # (matlab) implemented algorithms about initialization
â”œâ”€â”€ utils # (matlab) utility function for data processing and visualization in the pipeline
â”œâ”€â”€ eval_notebook # (python) scripts to evaluate performance between different methods
â”œâ”€â”€ main_BA.m # (matlab) script to demonstrate implemented method with bundle adjustment on `parking` data
â”œâ”€â”€ main_demo.m # (matlab) script to demonstrate implemented method without bundle adjustment for every dataset
â”œâ”€â”€ main_eval.m # (matlab) script to batch evaluate the implemented method with different features on `KITTI seq05` data
â”œâ”€â”€ data # 3 data sequences provided by VAME team and 2 customized sequences
â”œâ”€â”€ gifs # demonstration gifs
â”œâ”€â”€ README.md
â”œâ”€â”€ ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;h3 id=&#34;provided-datasets&#34;&gt;Provided datasets&lt;/h3&gt;

&lt;p&gt;Download data from &lt;a href=&#34;http://rpg.ifi.uzh.ch/teaching.html&#34; target=&#34;_blank&#34;&gt;RPG VAME course website&lt;/a&gt; and place them in the following structure&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;â”œâ”€â”€ data
â”‚   â”œâ”€â”€ kitti
â”‚   â””â”€â”€ malaga
â”‚   â””â”€â”€ parking
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;customized-datasets&#34;&gt;Customized datasets&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;For more details, you could refer to readme in following subfolder&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Jianhao-zheng/Visual-Odometry-Pipeline/tree/master/data/epfl_parking&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;epfl_parking&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Jianhao-zheng/Visual-Odometry-Pipeline/tree/master/data/lausanne_center_nav&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;lausanne_center_nav&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;related-repos&#34;&gt;Related repos&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hibetterheyj&#34; target=&#34;_blank&#34;&gt;hibetterheyj&lt;/a&gt;/&lt;strong&gt;&lt;a href=&#34;https://github.com/hibetterheyj/VideoIMUCapture-Android&#34; target=&#34;_blank&#34;&gt;VideoIMUCapture-Android&lt;/a&gt;&lt;/strong&gt; for camera calibration and image preprocessing (undistortion &amp;amp; resize)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Development of vision based algorithms to a window/balcony drone delivery</title>
      <link>https://yujie-he.github.io/project/2021-lis-drone-delivery/</link>
      <pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2021-lis-drone-delivery/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Semester Research Student&lt;/strong&gt; at &lt;a href=&#34;https://lis.epfl.ch/&#34; target=&#34;_blank&#34;&gt;Laboratory of Intelligent Systems (LIS)&lt;/a&gt;, EPFL since &lt;em&gt;Feb. 2021&lt;/em&gt;
Supervisor: &lt;a href=&#34;https://people.epfl.ch/valentin.wueest/?lang=en&#34; target=&#34;_blank&#34;&gt;Valentin WÃ¼est&lt;/a&gt; (PhD student), &lt;a href=&#34;https://people.epfl.ch/przemyslaw.kornatowski/?lang=en&#34; target=&#34;_blank&#34;&gt;Dr. Przemyslaw Mariusz Kornatowski&lt;/a&gt;, and &lt;a href=&#34;https://people.epfl.ch/dario.floreano&#34; target=&#34;_blank&#34;&gt;Prof. Dario Floreano&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;At the Laboratory of Intelligent Systems (LIS), passionate researchers are developing a human-friendly drone delivery system for last-cm delivery - &lt;a href=&#34;http://dronistics.epfl.ch&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Dronistics&lt;/strong&gt;&lt;/a&gt;. The system is composed of a safe drone called &lt;strong&gt;PackDrone&lt;/strong&gt; and software to control and monitor drones in real-time.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
  &lt;!-- &lt;img src=&#34;gearquad.jpg&#34; alt=&#34;gearquad_parcel&#34; style=&#34;zoom:12%;&#34; /&gt; --&gt;
  &lt;img src=&#34;https://dronistics.epfl.ch/img/PackDrone_deployed.jpg&#34; alt=&#34;PackDrone_deployed&#34;  width=&#34;200&#34;  /&gt;
  &lt;small&gt;
  &lt;b&gt;
    &lt;!-- Parcel placed above the cage allows the drone to transport parcels of various sizes without negative impact on lift --&gt;
     PackDrone can eliminate the damage from propellers or rotor blades with a foldable protective cage
  &lt;/b&gt;
  [Source: &lt;a href=&#34;http://dronistics.epfl.ch&#34; target=&#34;_blank&#34;&gt;Dronistics&lt;/a&gt;]
  &lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is the goal of this semester project?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our goal is to &lt;strong&gt;deliver to a balcony/window&lt;/strong&gt; which is &lt;strong&gt;tagged with a special symbol/pattern&lt;/strong&gt;. Moreover, the drone should be equipped with a system of &lt;strong&gt;collision avoidance&lt;/strong&gt; to prevent hitting a building.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Why motivates us to work on this project?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One vivid example is that in this special period of Covid-19, people are required to keep social distance while delivery work keeps operating. In contrast to large aircraft, window/balcony delivery with lightweight drone is a reasonable and effective solution to send valuable parcels such as medical supplies rapidly and safely.&lt;/p&gt;

&lt;h2 id=&#34;system-architecture&#34;&gt;System architecture&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Illustration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;./featured.jpg&#34; alt=&#34;experimental_drone&#34; width=&#34;600&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Software&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;./system_arch.jpg&#34; alt=&#34;system_arch&#34; width=&#34;600&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;center&gt;
  &lt;small&gt;
  &lt;b&gt;System architecture of the proposed drone delivery system&lt;/b&gt;
  &lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Visual fiducial marker evaluation
&lt;img src=&#34;./tag_evaluation.jpg&#34; alt=&#34;tag_evaluation&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Onboard test&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;./tag_drone_real.png&#34; alt=&#34;tag_drone_real&#34; width=&#34;600&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;final-presentation&#34;&gt;Final presentation&lt;/h2&gt;

&lt;!-- &lt;iframe src=&#34;https://drive.google.com/file/d/1LCtTQ2NFRRjhwrPHcfao5ApY6hocKZaQ/preview&#34; width=&#34;700&#34; height=&#34;400&#34;&gt;&lt;/iframe&gt; --&gt;

&lt;iframe src=&#34;https://drive.google.com/file/d/1VmY0fp5KuiljASgDYkci4Nhcj0Mt3HlK/preview&#34; width=&#34;700&#34; height=&#34;400&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Autonomous Navigation and Landing for Crazyflie</title>
      <link>https://yujie-he.github.io/project/2021-crazyflie-auto-nav/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2021-crazyflie-auto-nav/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Final project&lt;/strong&gt; in &lt;a href=&#34;https://edu.epfl.ch/coursebook/fr/aerial-robotics-MICRO-502&#34; target=&#34;_blank&#34;&gt;MICRO-502 Aerial robotics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Members: Yujie He, &lt;a href=&#34;https://github.com/Jianhao-zheng/&#34; target=&#34;_blank&#34;&gt;Jianhao Zheng&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kevinxqiu&#34; target=&#34;_blank&#34;&gt;Longlai Qiu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lecturer:  &lt;a href=&#34;https://people.epfl.ch/dario.floreano/?lang=en&#34; target=&#34;_blank&#34;&gt;Prof. Dario Floreano&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;goal-autonomous-navigation-and-landing-for-crazyflie&#34;&gt;Goal: Autonomous Navigation and Landing for Crazyflie&lt;/h2&gt;

&lt;p&gt;In this practical, we programed based on &lt;a href=&#34;https://www.bitcraze.io/products/crazyflie-2-1/&#34; target=&#34;_blank&#34;&gt;Crazyflie 2.1&lt;/a&gt; to find and precisely land on a platform with height of 10 cm by utilizing z reading from &lt;a href=&#34;https://www.bitcraze.io/products/flow-deck-v2/&#34; target=&#34;_blank&#34;&gt;flow deck&lt;/a&gt;. Additionally, We also utilized sensor readings from &lt;a href=&#34;https://www.bitcraze.io/products/multi-ranger-deck/&#34; target=&#34;_blank&#34;&gt;multi-ranger deck&lt;/a&gt; to avoid the obstacles presented in the environment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/blob/master/pics/cover.jpg&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/raw/master/pics/cover.jpg&#34; alt=&#34;cover&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Autonomous navigation &amp;amp; landing&lt;/th&gt;
&lt;th&gt;Workflow&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;âœ“ Local obstacle avoidance &lt;br&gt;âœ“ Grid-based coverage path planning &lt;br&gt;âœ“ Waypoint following &lt;br&gt;âœ“ A* search-based re-planning&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/master/pics/pipeline_final.png&#34; alt=&#34;pipeline_final&#34;  width=&#34;500&#34;/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;h3 id=&#34;features&#34;&gt;Features&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modular library for different tasks&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;â”œâ”€â”€ cf_load_params.py  # parameter setting
â”œâ”€â”€ cf_search.py       # searching functions such as, coverage planning, box edge detection, A* search
â”œâ”€â”€ cf_state_class.py  # state estimation class for the proposed task
â””â”€â”€ cf_utilis.py       # utility functions, such as live plotting
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Utilized &lt;code&gt;argparse&lt;/code&gt; for quick parameter adjustment and tuning&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Utilized &lt;code&gt;matplotlib&lt;/code&gt; for real-time visualization&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;structure&#34;&gt;Structure&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Code folder: &lt;a href=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/tree/master/code/crazyflie-lib-python/group_7&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;./code/crazyflie-lib-python/group_7/&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;.
â”œâ”€â”€ cf_load_params.py
â”œâ”€â”€ cf_search.py
â”œâ”€â”€ cf_state_class.py
â”œâ”€â”€ cf_utilis.py
â”œâ”€â”€ overall.py
â”œâ”€â”€ draw_traj_demo.py
â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ overall-20210530_1930_x.csv
â”‚   â”œâ”€â”€ overall-20210530_1930_x_half.csv
â”‚   â”œâ”€â”€ overall-20210530_1930_y.csv
â”‚   â””â”€â”€ overall-20210530_1930_y_half.cs
â””â”€â”€ readme.md
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;demo&#34;&gt;Demo&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;overall.py&lt;/code&gt;: overall pipeline from taking off to landing.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -x (float) for setting initial x position
# -y (float) for setting initial y position
# -v (bool) for enabling visualization
python overall.py -x 0.6 -y 0.6 -v
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/blob/master/pics/cf_land.gif&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/raw/master/pics/cf_land.gif&#34; alt=&#34;cf_land&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;draw_traj.py&lt;/code&gt;: x-y trajectory visualization with region annotation&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# --log_folder (str) for assigning input log folder
# --logname (str) for loding log file
# --img_folder (str) for assigning output image folder
# -n/--name (str) for assigning output image name
# --zone_anno (bool) for enabling region annotation
python draw_traj_demo.py --logname overall-20210530_1930 -n cf_demo --zone_anno
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/blob/master/pics/cf_demo.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/raw/master/pics/cf_demo.png&#34; alt=&#34;cf_demo&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The estimated values drift considerably after long flights. Moreover, the predicted starting position is significantly different from the starting point after the drone re-takes off.&lt;/p&gt;

&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Features&lt;/th&gt;
&lt;th&gt;Figures&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;âœ“ Size: 480 cm (W) Ã— 120 cm (H) &lt;/br&gt;âœ“ Starting &amp;amp; Landing pad&lt;/br&gt; - starting (x, y) = (60 cm, 60 cm)&lt;/br&gt; - landing pad randomly placed &lt;/br&gt;âœ“ Circular and rectangular obstacles&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hibetterheyj/Crazyflie_Auto_Navigation_Landing/master/pics/experimental_setup.png&#34; alt=&#34;experimental_setup&#34;  width=&#34;500&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;

&lt;p&gt;&lt;center&gt;
&lt;iframe width=&#34;560&#34; height=&#34;320&#34; src=&#34;https://www.youtube.com/embed/RP4-SlhOIUk&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;slide&#34;&gt;Slide&lt;/h2&gt;

&lt;p&gt;&lt;center&gt;
&lt;iframe src=&#34;https://drive.google.com/file/d/1vY_UMflVXOcUSOASHkGHsSTXCBmwrVhK/preview&#34; width=&#34;560&#34; height=&#34;320&#34;&gt;&lt;/iframe&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MULLS: Versatile LiDAR SLAM via Multi-metric Linear Least Square</title>
      <link>https://yujie-he.github.io/publication/2021_mulls_icra/</link>
      <pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/publication/2021_mulls_icra/</guid>
      <description>

&lt;h3 id=&#34;teaser&#34;&gt;Teaser&lt;/h3&gt;

&lt;p&gt;&lt;center&gt;
&lt;img src=&#34;./featured.png&#34; alt=&#34;MULLS&#34;  width=&#34;80%&#34;  /&gt;
&lt;small&gt;Pipeline of the multi-metric linear least square ICP&lt;/small&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;demo-video&#34;&gt;Demo video&lt;/h3&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;iframe width=&#34;600&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/85bGD55e3-0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;KITTI results&lt;/b&gt;&lt;/summary&gt;
&lt;div&gt;
  &lt;img src=&#34;https://github.com/YuePanEdward/MULLS/raw/main/assets/kitti_00_show.jpg&#34; alt=&#34;kitti_00_show&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/YuePanEdward/MULLS/raw/main/assets/kitti_01_show.jpg&#34; alt=&#34;kitti_01_show&#34; /&gt;
&lt;/div&gt;
&lt;/details&gt;&lt;/p&gt;

&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;

&lt;p&gt;If you find this project is useful, you may cite it as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@inproceedings{Pan2021ICRA,
  title={{MULLS: Versatile LiDAR SLAM via Multi-metric Linear Least Square}},
  author={Pan, Yue and Xiao, Pengchuan and He, Yujie and Shao, Zhenlei and Li, Zesong},
  booktitle={Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  year={2021},
  pages={1-8}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters</title>
      <link>https://yujie-he.github.io/publication/2020_tacf_iros/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/publication/2020_tacf_iros/</guid>
      <description>

&lt;!--
![TACF](featured.jpg)
&lt;small&gt;Comparison between baseline KCC tracker and the proposed TACF tracker&lt;/small&gt;
--&gt;

&lt;h3 id=&#34;videos&#34;&gt;Videos&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Presentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/XlFgz8h4dts&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tracking results demo&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/4IWKLmRoS38&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;

&lt;p&gt;If you find this project is useful, you may cite it as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@inproceedings{He2020IROS,
    title={{Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters}},
    author={He, Yujie and Fu, Changhong and Lin, Fuling and Li, Yiming and Lu, Peng},
    booktitle={Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS)},
    year={2020},
    pages={1575-1582}
 }
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Online Visual Object Tracking for UAV in Dynamic Environments</title>
      <link>https://yujie-he.github.io/project/2020-tracking4uav/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/project/2020-tracking4uav/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Undergraduate Research Assistant&lt;/strong&gt; since &lt;em&gt;Sep. 2018&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Investigated correlation filter (CF)-based &lt;strong&gt;visual object tracking&lt;/strong&gt; for unmanned aerial vehicles. By applying &lt;strong&gt;machine learning &amp;amp; deep learning&lt;/strong&gt; techniques, we have improved the existing trackers on overall tracking performance in challenging scenarios with real-time operational capability.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;papers-with-code&#34;&gt;Papers with code&lt;/h2&gt;

&lt;p&gt;Related work has been published in journals and conferences as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Proposed a lightweight and generalizable &lt;strong&gt;triple attention strategy&lt;/strong&gt; on CF-based framework by exploiting mutual independence of the appearance model and feature responses to implement real-time tracking for UAV.&lt;/p&gt;

&lt;p&gt;ðŸš© &lt;a href=&#34;../../publication/2020_tacf_iros/&#34;&gt;&lt;em&gt;Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters&lt;/em&gt;&lt;/a&gt; in &lt;strong&gt;IROS 2020&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Employed the adaptive &lt;strong&gt;GMSD-based context analysis&lt;/strong&gt; and &lt;strong&gt;dynamic weighted filters&lt;/strong&gt; for utilizing both contextual and historical information, and leveraged &lt;strong&gt;lightweight convolution features&lt;/strong&gt; to efficiently raise the tracking robustness.&lt;/p&gt;

&lt;p&gt;ðŸš© &lt;a href=&#34;../../publication/2020_mkct_ncaa/&#34;&gt;&lt;em&gt;Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters&lt;/em&gt;&lt;/a&gt; in &lt;strong&gt;Neural Computing and Applications&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Exploited the inter-frame information between prediction and backtracking phases for further incorporating the &lt;strong&gt;bidirectional incongruity error&lt;/strong&gt; into the CF learning.&lt;/p&gt;

&lt;p&gt;ðŸš© &lt;a href=&#34;../../publication/2020_bicf_icra/&#34;&gt;&lt;em&gt;BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking&lt;/em&gt;&lt;/a&gt; in &lt;strong&gt;ICRA 2020&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For more info, please refer to my &lt;a href=&#34;https://www.youtube.com/channel/UCGpK01NL0j3RkXpsODXm-Dg&#34; target=&#34;_blank&#34;&gt;YouTube channel&lt;/a&gt; and &lt;a href=&#34;https://github.com/hibetterheyj&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters</title>
      <link>https://yujie-he.github.io/publication/2020_mkct_ncaa/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yujie-he.github.io/publication/2020_mkct_ncaa/</guid>
      <description>

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;MKCT_workflow&#34; /&gt;
&lt;small&gt;Main structure of the proposed tracking approach&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;

&lt;p&gt;If you find this project is useful, you may cite it as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-tex&#34;&gt;@article{Fu2020NCAA,
    title={{Robust Multi-Kernelized Correlators for UAV Tracking with Adaptive Context Analysis and Dynamic Weighted Filters}},
    author={Fu, Changhong and He, Yujie and Lin, Fuling and Xiong, Weijiang},
    journal={Neural Computing and Applications},
    pages={1-17},
    year={2020}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
